
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Free RAG Application for API Documentation\n",
        "\n",
        "This notebook creates a complete RAG (Retrieval-Augmented Generation) application using **100% FREE** models:\n",
        "- **LLM**: Google's Flan-T5 (free)\n",
        "- **Embeddings**: Sentence Transformers (free)\n",
        "- **Vector DB**: ChromaDB (free)\n",
        "- **Interface**: Gradio (free)\n",
        "\n",
        "## üìã Instructions:\n",
        "1. Run all cells in order\n",
        "2. Upload your API documentation files\n",
        "3. Ask questions about your API!\n",
        "\n",
        "**No API keys required!** üéâ"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers torch sentence-transformers chromadb gradio PyPDF2 python-docx markdown tiktoken"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "from typing import List, Dict, Optional\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import tiktoken\n",
        "import markdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FreeRAGConfig:\n",
        "    \"\"\"Configuration for free RAG application\"\"\"\n",
        "    \n",
        "    # Model Configuration\n",
        "    LLM_MODEL = \"google/flan-t5-base\"  # Free, good performance\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Fast and free\n",
        "    \n",
        "    # Application Settings\n",
        "    MAX_CHUNK_SIZE = 800\n",
        "    CHUNK_OVERLAP = 150\n",
        "    MAX_TOKENS = 512\n",
        "    TEMPERATURE = 0.7\n",
        "    \n",
        "    # Vector Database\n",
        "    COLLECTION_NAME = \"api_docs_free\"\n",
        "    \n",
        "    # Supported file types\n",
        "    SUPPORTED_FILE_TYPES = ['.txt', '.md', '.pdf', '.docx']\n",
        "\n",
        "print(\"‚öôÔ∏è Configuration loaded!\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FreeDocumentProcessor:\n",
        "    \"\"\"Document processing for free RAG application\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.config = FreeRAGConfig()\n",
        "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    \n",
        "    def load_document(self, file_path: str, file_type: str) -> str:\n",
        "        \"\"\"Load document content based on file type\"\"\"\n",
        "        try:\n",
        "            if file_type == '.txt':\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            elif file_type == '.md':\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            elif file_type == '.pdf':\n",
        "                text = \"\"\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    pdf_reader = PyPDF2.PdfReader(f)\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "            elif file_type == '.docx':\n",
        "                doc = Document(file_path)\n",
        "                text = \"\"\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + \"\\n\"\n",
        "                return text\n",
        "            else:\n",
        "                return \"Unsupported file type\"\n",
        "        except Exception as e:\n",
        "            return f\"Error loading file: {str(e)}\"\n",
        "    \n",
        "    def chunk_text(self, text: str, filename: str) -> List[Dict]:\n",
        "        \"\"\"Split text into chunks\"\"\"\n",
        "        # Clean text\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        \n",
        "        # Split into sentences\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        \n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        current_tokens = 0\n",
        "        \n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = len(self.encoding.encode(sentence))\n",
        "            \n",
        "            if current_tokens + sentence_tokens > self.config.MAX_CHUNK_SIZE and current_chunk:\n",
        "                chunks.append({\n",
        "                    'text': current_chunk.strip(),\n",
        "                    'source': filename,\n",
        "                    'chunk_id': len(chunks),\n",
        "                    'tokens': current_tokens\n",
        "                })\n",
        "                \n",
        "                # Start new chunk with overlap\n",
        "                overlap_text = current_chunk[-self.config.CHUNK_OVERLAP:] if len(current_chunk) > self.config.CHUNK_OVERLAP else current_chunk\n",
        "                current_chunk = overlap_text + \" \" + sentence\n",
        "                current_tokens = len(self.encoding.encode(current_chunk))\n",
        "            else:\n",
        "                current_chunk += \" \" + sentence\n",
        "                current_tokens += sentence_tokens\n",
        "        \n",
        "        # Add final chunk\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'text': current_chunk.strip(),\n",
        "                'source': filename,\n",
        "                'chunk_id': len(chunks),\n",
        "                'tokens': current_tokens\n",
        "            })\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "print(\"üìÑ Document processor ready!\")"
      ],
      "metadata": {
        "id": "document_processor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FreeVectorStore:\n",
        "    \"\"\"Free vector store using ChromaDB and Sentence Transformers\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.config = FreeRAGConfig()\n",
        "        \n",
        "        # Initialize embedding model\n",
        "        print(\"üîÑ Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer(self.config.EMBEDDING_MODEL)\n",
        "        print(\"‚úÖ Embedding model loaded!\")\n",
        "        \n",
        "        # Initialize ChromaDB\n",
        "        self.client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "        \n",
        "        # Create or get collection\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=self.config.COLLECTION_NAME)\n",
        "            print(f\"üìö Existing collection '{self.config.COLLECTION_NAME}' loaded\")\n",
        "        except:\n",
        "            self.collection = self.client.create_collection(name=self.config.COLLECTION_NAME)\n",
        "            print(f\"üìö New collection '{self.config.COLLECTION_NAME}' created\")\n",
        "    \n",
        "    def add_documents(self, chunks: List[Dict]) -> bool:\n",
        "        \"\"\"Add document chunks to vector store\"\"\"\n",
        "        try:\n",
        "            texts = [chunk['text'] for chunk in chunks]\n",
        "            \n",
        "            # Generate embeddings\n",
        "            print(f\"üîÑ Generating embeddings for {len(texts)} chunks...\")\n",
        "            embeddings = self.embedding_model.encode(texts).tolist()\n",
        "            \n",
        "            # Prepare metadata and IDs\n",
        "            metadatas = [{\n",
        "                'source': chunk['source'],\n",
        "                'chunk_id': chunk['chunk_id'],\n",
        "                'tokens': chunk['tokens']\n",
        "            } for chunk in chunks]\n",
        "            \n",
        "            ids = [f\"{chunk['source']}_{chunk['chunk_id']}\" for chunk in chunks]\n",
        "            \n",
        "            # Add to collection\n",
        "            self.collection.add(\n",
        "                documents=texts,\n",
        "                embeddings=embeddings,\n",
        "                metadatas=metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úÖ Added {len(texts)} chunks to vector store\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error adding documents: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def search_similar_documents(self, query: str, n_results: int = 5) -> List[Dict]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        try:\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.embedding_model.encode([query]).tolist()[0]\n",
        "            \n",
        "            # Search in collection\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=[query_embedding],\n",
        "                n_results=n_results,\n",
        "                include=['documents', 'metadatas', 'distances']\n",
        "            )\n",
        "            \n",
        "            # Format results\n",
        "            formatted_results = []\n",
        "            if results['documents'] and results['documents'][0]:\n",
        "                for i in range(len(results['documents'][0])):\n",
        "                    formatted_results.append({\n",
        "                        'text': results['documents'][0][i],\n",
        "                        'metadata': results['metadatas'][0][i],\n",
        "                        'similarity_score': 1 - results['distances'][0][i]\n",
        "                    })\n",
        "            \n",
        "            return formatted_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error searching documents: {str(e)}\")\n",
        "            return []\n",
        "    \n",
        "    def get_collection_info(self) -> Dict:\n",
        "        \"\"\"Get collection information\"\"\"\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            return {\n",
        "                'document_count': count,\n",
        "                'status': 'active' if count > 0 else 'empty'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'document_count': 0,\n",
        "                'status': 'error',\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "print(\"üóÑÔ∏è Vector store ready!\")"
      ],
      "metadata": {
        "id": "vector_store"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FreeRAGPipeline:\n",
        "    \"\"\"Free RAG pipeline using Flan-T5\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.config = FreeRAGConfig()\n",
        "        \n",
        "        # Initialize LLM\n",
        "        print(\"üîÑ Loading language model...\")\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.LLM_MODEL)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.config.LLM_MODEL)\n",
        "        self.model.to(device)\n",
        "        \n",
        "        self.generator = pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            device=0 if torch.cuda.is_available() else -1,\n",
        "            max_length=self.config.MAX_TOKENS,\n",
        "            temperature=self.config.TEMPERATURE,\n",
        "            do_sample=True\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Language model loaded on {device}!\")\n",
        "        \n",
        "        # Initialize vector store\n",
        "        self.vector_store = FreeVectorStore()\n",
        "    \n",
        "    def process_query(self, query: str, n_results: int = 3) -> Dict:\n",
        "        \"\"\"Process user query and generate response\"\"\"\n",
        "        try:\n",
        "            # Retrieve relevant documents\n",
        "            relevant_docs = self.vector_store.search_similar_documents(query, n_results)\n",
        "            \n",
        "            if not relevant_docs:\n",
        "                return {\n",
        "                    'response': \"I don't have any relevant information to answer your question. Please upload some API documentation first.\",\n",
        "                    'sources': [],\n",
        "                    'confidence': 0.0\n",
        "                }\n",
        "            \n",
        "            # Prepare context\n",
        "            context = self._prepare_context(relevant_docs)\n",
        "            \n",
        "            # Generate response\n",
        "            response = self._generate_response(query, context)\n",
        "            \n",
        "            # Extract sources\n",
        "            sources = self._extract_sources(relevant_docs)\n",
        "            \n",
        "            # Calculate confidence\n",
        "            confidence = self._calculate_confidence(relevant_docs)\n",
        "            \n",
        "            return {\n",
        "                'response': response,\n",
        "                'sources': sources,\n",
        "                'confidence': confidence,\n",
        "                'retrieved_chunks': len(relevant_docs)\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'response': f\"An error occurred: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0,\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    def _prepare_context(self, relevant_docs: List[Dict]) -> str:\n",
        "        \"\"\"Prepare context from retrieved documents\"\"\"\n",
        "        context_parts = []\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            source = doc['metadata'].get('source', 'Unknown')\n",
        "            text = doc['text'][:500]  # Limit context length\n",
        "            context_parts.append(f\"Source {i} ({source}): {text}\")\n",
        "        \n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "    \n",
        "    def _generate_response(self, query: str, context: str) -> str:\n",
        "        \"\"\"Generate response using Flan-T5\"\"\"\n",
        "        prompt = f\"\"\"Based on the following API documentation context, answer the question clearly and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Limit prompt length\n",
        "            if len(prompt) > 1000:\n",
        "                prompt = prompt[:1000] + \"...\"\n",
        "            \n",
        "            result = self.generator(prompt, max_length=self.config.MAX_TOKENS, num_return_sequences=1)\n",
        "            response = result[0]['generated_text'].strip()\n",
        "            \n",
        "            # Clean up response\n",
        "            if response.startswith(prompt):\n",
        "                response = response[len(prompt):].strip()\n",
        "            \n",
        "            return response if response else \"I couldn't generate a proper response. Please try rephrasing your question.\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "    \n",
        "    def _extract_sources(self, relevant_docs: List[Dict]) -> List[str]:\n",
        "        \"\"\"Extract unique sources\"\"\"\n",
        "        sources = []\n",
        "        seen = set()\n",
        "        \n",
        "        for doc in relevant_docs:\n",
        "            source = doc['metadata'].get('source', 'Unknown')\n",
        "            if source not in seen:\n",
        "                sources.append(source)\n",
        "                seen.add(source)\n",
        "        \n",
        "        return sources\n",
        "    \n",
        "    def _calculate_confidence(self, relevant_docs: List[Dict]) -> float:\n",
        "        \"\"\"Calculate confidence score\"\"\"\n",
        "        if not relevant_docs:\n",
        "            return 0.0\n",
        "        \n",
        "        scores = [doc.get('similarity_score', 0.0) for doc in relevant_docs]\n",
        "        avg_score = sum(scores) / len(scores)\n",
        "        \n",
        "        return round(min(avg_score + 0.1, 1.0), 3)\n",
        "\n",
        "print(\"ü§ñ RAG pipeline ready!\")"
      ],
      "metadata": {
        "id": "rag_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RAG system\n",
        "print(\"üöÄ Initializing Free RAG System...\")\n",
        "print(\"This may take a few minutes to download models...\")\n",
        "\n",
        "# Initialize components\n",
        "document_processor = FreeDocumentProcessor()\n",
        "rag_pipeline = FreeRAGPipeline()\n",
        "\n",
        "print(\"\\nüéâ Free RAG System Ready!\")\n",
        "print(\"üìä System Info:\")\n",
        "print(f\"   üíæ GPU Available: {torch.cuda.is_available()}\")\n",
        "print(f\"   üß† LLM Model: {FreeRAGConfig.LLM_MODEL}\")\n",
        "print(f\"   üîç Embedding Model: {FreeRAGConfig.EMBEDDING_MODEL}\")\n",
        "print(f\"   üìö Vector Store: ChromaDB\")"
      ],
      "metadata": {
        "id": "initialize_system"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_and_process_files(files):\n",
        "    \"\"\"Process uploaded files\"\"\"\n",
        "    if not files:\n",
        "        return \"‚ùå No files uploaded!\", \"\"\n",
        "    \n",
        "    results = []\n",
        "    total_chunks = 0\n",
        "    \n",
        "    for file in files:\n",
        "        try:\n",
        "            # Get file info\n",
        "            filename = os.path.basename(file.name)\n",
        "            file_ext = os.path.splitext(filename)[1].lower()\n",
        "            \n",
        "            if file_ext not in FreeRAGConfig.SUPPORTED_FILE_TYPES:\n",
        "                results.append(f\"‚ùå {filename}: Unsupported file type\")\n",
        "                continue\n",
        "            \n",
        "            # Load document\n",
        "            content = document_processor.load_document(file.name, file_ext)\n",
        "            \n",
        "            if content.startswith(\"Error\") or content.startswith(\"Unsupported\"):\n",
        "                results.append(f\"‚ùå {filename}: {content}\")\n",
        "                continue\n",
        "            \n",
        "            # Process into chunks\n",
        "            chunks = document_processor.chunk_text(content, filename)\n",
        "            \n",
        "            if not chunks:\n",
        "                results.append(f\"‚ùå {filename}: No content extracted\")\n",
        "                continue\n",
        "            \n",
        "            # Add to vector store\n",
        "            success = rag_pipeline.vector_store.add_documents(chunks)\n",
        "            \n",
        "            if success:\n",
        "                total_chunks += len(chunks)\n",
        "                results.append(f\"‚úÖ {filename}: {len(chunks)} chunks processed\")\n",
        "            else:\n",
        "                results.append(f\"‚ùå {filename}: Failed to add to vector store\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            results.append(f\"‚ùå {filename}: Error - {str(e)}\")\n",
        "    \n",
        "    # Get collection info\n",
        "    collection_info = rag_pipeline.vector_store.get_collection_info()\n",
        "    \n",
        "    summary = f\"\\nüìä Processing Summary:\\n\"\n",
        "    summary += f\"   üìÅ Files processed: {len([r for r in results if r.startswith('‚úÖ')])}\\n\"\n",
        "    summary += f\"   üìÑ Total chunks: {total_chunks}\\n\"\n",
        "    summary += f\"   üóÑÔ∏è Total documents in database: {collection_info.get('document_count', 0)}\\n\"\n",
        "    \n",
        "    return \"\\n\".join(results) + summary, f\"Database: {collection_info.get('document_count', 0)} documents\"\n",
        "\n",
        "def ask_question(question, history):\n",
        "    \"\"\"Process user question\"\"\"\n",
        "    if not question.strip():\n",
        "        return history, \"\"\n",
        "    \n",
        "    # Check if database has documents\n",
        "    collection_info = rag_pipeline.vector_store.get_collection_info()\n",
        "    if collection_info.get('document_count', 0) == 0:\n",
        "        response = \"‚ö†Ô∏è No documents in the database. Please upload some API documentation first!\"\n",
        "        history.append([question, response])\n",
        "        return history, \"\"\n",
        "    \n",
        "    # Process question\n",
        "    try:\n",
        "        result = rag_pipeline.process_query(question)\n",
        "        \n",
        "        response = result['response']\n",
        "        \n",
        "        # Add metadata\n",
        "        if result.get('sources'):\n",
        "            response += f\"\\n\\nüìö **Sources:** {', '.join(result['sources'])}\"\n",
        "        \n",
        "        if result.get('confidence'):\n",
        "            confidence_emoji = \"üü¢\" if result['confidence'] > 0.7 else \"üü°\" if result['confidence'] > 0.4 else \"üî¥\"\n",
        "            response += f\"\\n{confidence_emoji} **Confidence:** {result['confidence']:.1%}\"\n",
        "        \n",
        "        history.append([question, response])\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_response = f\"‚ùå Error processing question: {str(e)}\"\n",
        "        history.append([question, error_response])\n",
        "    \n",
        "    return history, \"\"\n",
        "\n",
        "print(\"üîß Helper functions ready!\")"
      ],
      "metadata": {
        "id": "helper_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio Interface\n",
        "with gr.Blocks(title=\"Free RAG API Documentation Assistant\", theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üöÄ Free RAG API Documentation Assistant\n",
        "        \n",
        "        Upload your API documentation and ask questions! This app uses **100% free models**:\n",
        "        - üß† **LLM**: Google Flan-T5 (free)\n",
        "        - üîç **Embeddings**: Sentence Transformers (free) \n",
        "        - üóÑÔ∏è **Vector DB**: ChromaDB (free)\n",
        "        \n",
        "        **Supported formats**: TXT, MD, PDF, DOCX\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    with gr.Tab(\"üì§ Upload Documents\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                file_upload = gr.Files(\n",
        "                    label=\"Upload API Documentation Files\",\n",
        "                    file_types=[\".txt\", \".md\", \".pdf\", \".docx\"],\n",
        "                    file_count=\"multiple\"\n",
        "                )\n",
        "                upload_btn = gr.Button(\"üìÑ Process Documents\", variant=\"primary\")\n",
        "            \n",
        "            with gr.Column():\n",
        "                upload_status = gr.Textbox(\n",
        "                    label=\"Processing Status\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "                db_status = gr.Textbox(\n",
        "                    label=\"Database Status\",\n",
        "                    interactive=False\n",
        "                )\n",
        "    \n",
        "    with gr.Tab(\"üí¨ Ask Questions\"):\n",
        "        chatbot = gr.